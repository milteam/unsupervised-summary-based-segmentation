{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def determine_text_length_bins(texts, num_bins=5):\n",
    "    # Tokenize texts and count words\n",
    "    word_counts = [len(word_tokenize(text)) for text in texts]\n",
    "    \n",
    "    # Calculate bin size based on the range of word counts\n",
    "    min_word_count = min(word_counts)\n",
    "    max_word_count = max(word_counts)\n",
    "    bin_size = (max_word_count - min_word_count) // num_bins\n",
    "    \n",
    "    # Initialize bins\n",
    "    bin_ranges = [(min_word_count + i * bin_size, min_word_count + (i + 1) * bin_size) for i in range(num_bins)]\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(word_counts, bins=[bin_range[0] for bin_range in bin_ranges] + [bin_ranges[-1][1]], rwidth=0.8)\n",
    "    \n",
    "    # Label the plot\n",
    "    plt.xticks([bin_range[0] for bin_range in bin_ranges], [f\"{bin_range[0]}-{bin_range[1]}\" for bin_range in bin_ranges])\n",
    "    plt.xlabel(\"Text Length Range (Number of Words)\")\n",
    "    plt.ylabel(\"Number of Texts\")\n",
    "    plt.title(\"Text Length Distribution\")\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_from_disk('/data/shared/datasets/nlp/superseg_test_summarized.hf/')\n",
    "# ds = load_from_disk('/data/shared/datasets/nlp/superseg_test_summarized_bart.hf/')\n",
    "# ds = load_from_disk('/data/shared/datasets/nlp/superseg_train_summarized.hf/')\n",
    "ds = load_from_disk('/data/shared/datasets/nlp/ami_summarized_bart.hf/')\n",
    "# ds = load_from_disk('/data/shared/datasets/nlp/dialseg711_summarized.hf/')\n",
    "# ds = load_from_disk('/data/shared/datasets/nlp/tiage_train_summarized.hf/')\n",
    "# ds = load_from_disk('/data/shared/datasets/nlp/wiki727test_summarized.hf/').select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.filter(lambda x: len(word_tokenize(' '.join(x['sections']))) > 1460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(a) for a in ds['sections']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "determine_text_length_bins(texts, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../lib/pipelines/')\n",
    "sys.path.append('../lib/pipelines/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utilities.tiling import TopicTilingModel, classify_borders\n",
    "from utilities.general import calc_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def determine_text_length_bins_with_custom_values(texts, custom_values, num_bins=5):\n",
    "    # Tokenize texts and count words\n",
    "    word_counts = [len(word_tokenize(text)) for text in texts]\n",
    "\n",
    "    # Calculate bin size based on the range of word counts\n",
    "    min_word_count = min(word_counts)\n",
    "    max_word_count = max(word_counts)\n",
    "    bin_size = (max_word_count - min_word_count) // num_bins\n",
    "\n",
    "    # Initialize bins\n",
    "    bin_ranges = [(min_word_count + i * bin_size, min_word_count + (i + 1) * bin_size) for i in range(num_bins)]\n",
    "\n",
    "    # Initialize a dictionary to store custom values for each bin\n",
    "    bin_custom_values = {bin_range: [] for bin_range in bin_ranges}\n",
    "\n",
    "    # Assign custom values to bins based on text length\n",
    "    for i, word_count in enumerate(word_counts):\n",
    "        for bin_range in bin_ranges:\n",
    "            if bin_range[0] <= word_count <= bin_range[1]:\n",
    "                bin_custom_values[bin_range].append(custom_values[i])\n",
    "\n",
    "    # Create a bar plot using custom values for each bin\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bins = [sum(bin_custom_values[bin_range])/(len(bin_custom_values[bin_range])+0.01) for bin_range in bin_ranges]\n",
    "    print(sum(bins)/len(bins))\n",
    "    plt.barh(range(len(bin_ranges)), bins)\n",
    "\n",
    "    # Label the plot\n",
    "    plt.yticks(range(len(bin_ranges)), [f\"{bin_range[0]}-{bin_range[1]}\" for bin_range in bin_ranges])\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Text Length Range (Number of Words)\")\n",
    "    plt.title(\"Text Length Distribution with Custom Values\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return the custom values for each bin based on text length\n",
    "    return bin_custom_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(example):\n",
    "    pred = classify_borders(example['embeddings'], 30, 0.6, 1, 30)\n",
    "    wd, pk, f1 = calc_metric(example, pred)\n",
    "    example['wd'] = float(wd)\n",
    "    example['pk'] = float(pk)\n",
    "    example['f1'] = float(f1)\n",
    "    return example\n",
    "ds = ds.map(get_scores)\n",
    "\n",
    "bin_custom_values = determine_text_length_bins_with_custom_values(texts, ds['wd'], num_bins=10)\n",
    "\n",
    "# Print the custom values for each bin based on text length\n",
    "for bin_range, bin_values in bin_custom_values.items():\n",
    "    print(f\"Bin {bin_range}: {bin_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_borders_sumseg(example, tiling_model, plot):\n",
    "    probabilities = example['probs']\n",
    "    boundaries = tiling_model.transform(probabilities, gold_boundaries=example['boundaries'], plot=plot)\n",
    "    return boundaries\n",
    "\n",
    "params = {\n",
    "            'window_size': 15, \n",
    "            'threshold': 0.6, \n",
    "            'smoothing_passes': 0, \n",
    "            'smoothing_window': 0,\n",
    "            'n_smooth_savgol': 3,\n",
    "            'savgol_k': 1/3,\n",
    "            'polyorder': 3\n",
    "        }\n",
    "tiling_model = TopicTilingModel(**params)\n",
    "\n",
    "def get_scores(example):\n",
    "    # pred = classify_borders(example['embeddings'], 27, 0.65, 2, 6)\n",
    "    pred = _get_borders_sumseg(example, tiling_model, False)\n",
    "    wd, pk, f1 = calc_metric(example, pred)\n",
    "    example['wd'] = wd\n",
    "    example['pk'] = pk\n",
    "    example['f1'] = f1\n",
    "    return example\n",
    "\n",
    "ds = ds.map(get_scores)\n",
    "\n",
    "bin_custom_values = determine_text_length_bins_with_custom_values(texts, ds['wd'], num_bins=10)\n",
    "for bin_range, bin_values in bin_custom_values.items():\n",
    "    print(f\"Bin {bin_range}: {bin_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def calculate_statistics(ds, verbose=True):\n",
    "    def mean(array):\n",
    "        if len(array):\n",
    "            return sum(array) / len(array)\n",
    "        else:\n",
    "            raise ValueError('Failed to calculate mean metric, check dataset!')\n",
    "    \n",
    "    doc_utterances = ds['sections']\n",
    "    doc_texts = [' '.join(text) for text in doc_utterances]\n",
    "    \n",
    "    # create sections\n",
    "    sections = []\n",
    "    for example in ds:\n",
    "        section = ''\n",
    "        utterances = example['sections']\n",
    "        boundaries = example['boundaries']\n",
    "        for i in range(len(boundaries)):\n",
    "            if boundaries[i] == '0':\n",
    "                section += ' ' + utterances[i]\n",
    "            else:\n",
    "                sections.append(section.strip())\n",
    "                section = utterances[i]\n",
    "    \n",
    "    # avg # sections in doc\n",
    "    boundaries  = ds['boundaries']\n",
    "    avg_segment_length = mean([len(b) / (b.count('1') + 1) for b in boundaries])\n",
    "\n",
    "    # avg # words in section\n",
    "    avg_n_words_in_section = mean([len(word_tokenize(section)) for section in sections])\n",
    "    \n",
    "    # min / max / avg # words in doc\n",
    "    n_words_in_doc = [len(word_tokenize(text)) for text in doc_texts]\n",
    "    avg_n_words_in_doc = mean(n_words_in_doc)\n",
    "    min_n_words_in_doc = min(n_words_in_doc)\n",
    "    max_n_words_in_doc = max(n_words_in_doc)\n",
    "    \n",
    "    # avg # utterances in doc\n",
    "    avg_n_utterances_in_doc = mean([len(doc_utterance) for doc_utterance in doc_utterances])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'# docs: {len(ds)}')\n",
    "        print(f'min / avg / max # words in doc: {min_n_words_in_doc:.0f} / {avg_n_words_in_doc:.0f} / {max_n_words_in_doc:.0f}')\n",
    "        print(f'avg # words in section: {avg_n_words_in_section:.0f}')\n",
    "        print(f'avg # utterances in doc: {avg_n_utterances_in_doc:.0f}')\n",
    "        print(f'avg # utterances in section: {avg_segment_length:.0f}')\n",
    "        \n",
    "    return avg_segment_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_statistics(ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between # utterances and # sentences in summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_utt_sent_sum(example):\n",
    "    example['n_utterances'] = len(example['sections'])\n",
    "    example['n_sentences_summary'] = len(example['splitted_summary'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_from_disk('/data/shared/datasets/nlp/ami_summarized_bart.hf/')\n",
    "# ds = load_from_disk('/home/user/clustering/neuroclustering/lib/pipelines/runs/sumseg_run_20231015_224151/test_embedded_dataset') #qmsum\n",
    "# ds = load_from_disk('/home/user/clustering/neuroclustering/lib/pipelines/runs/sumseg_run_20231015_225338/test_embedded_dataset') # superdialseg\n",
    "# ds = load_from_disk('/home/user/clustering/neuroclustering/lib/pipelines/runs/sumseg_run_20231015_225758/test_embedded_dataset') # qmsum\n",
    "ds = load_from_disk('/home/user/clustering/neuroclustering/lib/pipelines/runs/sumseg_run_20231015_230135/test_embedded_dataset') # dialseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(correlation_utt_sent_sum)\n",
    "n_utterances = ds['n_utterances']\n",
    "n_sentences_summary = ds['n_sentences_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(n_utterances, n_sentences_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(n_utterances, n_sentences_summary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
